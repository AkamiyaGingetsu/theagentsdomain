# coding: utf-8

"""
    OpenAI API

    The OpenAI REST API. Please see https://platform.openai.com/docs/api-reference for more details.  # noqa: E501

    OpenAPI spec version: 2.0.0
    
    Generated by: https://github.com/swagger-api/swagger-codegen.git
"""

import pprint
import re  # noqa: F401

import six

class CreateChatCompletionRequest(object):
    """NOTE: This class is auto generated by the swagger code generator program.

    Do not edit the class manually.
    """
    """
    Attributes:
      swagger_types (dict): The key is attribute name
                            and the value is attribute type.
      attribute_map (dict): The key is attribute name
                            and the value is json key in definition.
    """
    swagger_types = {
        'messages': 'list[ChatCompletionRequestMessage]',
        'model': 'AnyOfCreateChatCompletionRequestModel',
        'frequency_penalty': 'float',
        'logit_bias': 'dict(str, int)',
        'logprobs': 'bool',
        'top_logprobs': 'int',
        'max_tokens': 'int',
        'n': 'int',
        'presence_penalty': 'float',
        'response_format': 'CreateChatCompletionRequestResponseFormat',
        'seed': 'int',
        'stop': 'OneOfCreateChatCompletionRequestStop',
        'stream': 'bool',
        'stream_options': 'ChatCompletionStreamOptions',
        'temperature': 'float',
        'top_p': 'float',
        'tools': 'list[ChatCompletionTool]',
        'tool_choice': 'ChatCompletionToolChoiceOption',
        'user': 'str',
        'function_call': 'OneOfCreateChatCompletionRequestFunctionCall',
        'functions': 'list[ChatCompletionFunctions]'
    }

    attribute_map = {
        'messages': 'messages',
        'model': 'model',
        'frequency_penalty': 'frequency_penalty',
        'logit_bias': 'logit_bias',
        'logprobs': 'logprobs',
        'top_logprobs': 'top_logprobs',
        'max_tokens': 'max_tokens',
        'n': 'n',
        'presence_penalty': 'presence_penalty',
        'response_format': 'response_format',
        'seed': 'seed',
        'stop': 'stop',
        'stream': 'stream',
        'stream_options': 'stream_options',
        'temperature': 'temperature',
        'top_p': 'top_p',
        'tools': 'tools',
        'tool_choice': 'tool_choice',
        'user': 'user',
        'function_call': 'function_call',
        'functions': 'functions'
    }

    def __init__(self, messages=None, model=None, frequency_penalty=0, logit_bias=None, logprobs=False, top_logprobs=None, max_tokens=None, n=1, presence_penalty=0, response_format=None, seed=None, stop=None, stream=False, stream_options=None, temperature=1, top_p=1, tools=None, tool_choice=None, user=None, function_call=None, functions=None):  # noqa: E501
        """CreateChatCompletionRequest - a model defined in Swagger"""  # noqa: E501
        self._messages = None
        self._model = None
        self._frequency_penalty = None
        self._logit_bias = None
        self._logprobs = None
        self._top_logprobs = None
        self._max_tokens = None
        self._n = None
        self._presence_penalty = None
        self._response_format = None
        self._seed = None
        self._stop = None
        self._stream = None
        self._stream_options = None
        self._temperature = None
        self._top_p = None
        self._tools = None
        self._tool_choice = None
        self._user = None
        self._function_call = None
        self._functions = None
        self.discriminator = None
        self.messages = messages
        self.model = model
        if frequency_penalty is not None:
            self.frequency_penalty = frequency_penalty
        if logit_bias is not None:
            self.logit_bias = logit_bias
        if logprobs is not None:
            self.logprobs = logprobs
        if top_logprobs is not None:
            self.top_logprobs = top_logprobs
        if max_tokens is not None:
            self.max_tokens = max_tokens
        if n is not None:
            self.n = n
        if presence_penalty is not None:
            self.presence_penalty = presence_penalty
        if response_format is not None:
            self.response_format = response_format
        if seed is not None:
            self.seed = seed
        if stop is not None:
            self.stop = stop
        if stream is not None:
            self.stream = stream
        if stream_options is not None:
            self.stream_options = stream_options
        if temperature is not None:
            self.temperature = temperature
        if top_p is not None:
            self.top_p = top_p
        if tools is not None:
            self.tools = tools
        if tool_choice is not None:
            self.tool_choice = tool_choice
        if user is not None:
            self.user = user
        if function_call is not None:
            self.function_call = function_call
        if functions is not None:
            self.functions = functions

    @property
    def messages(self):
        """Gets the messages of this CreateChatCompletionRequest.  # noqa: E501

        A list of messages comprising the conversation so far. [Example Python code](https://cookbook.openai.com/examples/how_to_format_inputs_to_chatgpt_models).  # noqa: E501

        :return: The messages of this CreateChatCompletionRequest.  # noqa: E501
        :rtype: list[ChatCompletionRequestMessage]
        """
        return self._messages

    @messages.setter
    def messages(self, messages):
        """Sets the messages of this CreateChatCompletionRequest.

        A list of messages comprising the conversation so far. [Example Python code](https://cookbook.openai.com/examples/how_to_format_inputs_to_chatgpt_models).  # noqa: E501

        :param messages: The messages of this CreateChatCompletionRequest.  # noqa: E501
        :type: list[ChatCompletionRequestMessage]
        """
        if messages is None:
            raise ValueError("Invalid value for `messages`, must not be `None`")  # noqa: E501

        self._messages = messages

    @property
    def model(self):
        """Gets the model of this CreateChatCompletionRequest.  # noqa: E501

        ID of the model to use. See the [model endpoint compatibility](/docs/models/model-endpoint-compatibility) table for details on which models work with the Chat API.  # noqa: E501

        :return: The model of this CreateChatCompletionRequest.  # noqa: E501
        :rtype: AnyOfCreateChatCompletionRequestModel
        """
        return self._model

    @model.setter
    def model(self, model):
        """Sets the model of this CreateChatCompletionRequest.

        ID of the model to use. See the [model endpoint compatibility](/docs/models/model-endpoint-compatibility) table for details on which models work with the Chat API.  # noqa: E501

        :param model: The model of this CreateChatCompletionRequest.  # noqa: E501
        :type: AnyOfCreateChatCompletionRequestModel
        """
        if model is None:
            raise ValueError("Invalid value for `model`, must not be `None`")  # noqa: E501

        self._model = model

    @property
    def frequency_penalty(self):
        """Gets the frequency_penalty of this CreateChatCompletionRequest.  # noqa: E501

        Number between -2.0 and 2.0. Positive values penalize new tokens based on their existing frequency in the text so far, decreasing the model's likelihood to repeat the same line verbatim.  [See more information about frequency and presence penalties.](/docs/guides/text-generation/parameter-details)   # noqa: E501

        :return: The frequency_penalty of this CreateChatCompletionRequest.  # noqa: E501
        :rtype: float
        """
        return self._frequency_penalty

    @frequency_penalty.setter
    def frequency_penalty(self, frequency_penalty):
        """Sets the frequency_penalty of this CreateChatCompletionRequest.

        Number between -2.0 and 2.0. Positive values penalize new tokens based on their existing frequency in the text so far, decreasing the model's likelihood to repeat the same line verbatim.  [See more information about frequency and presence penalties.](/docs/guides/text-generation/parameter-details)   # noqa: E501

        :param frequency_penalty: The frequency_penalty of this CreateChatCompletionRequest.  # noqa: E501
        :type: float
        """

        self._frequency_penalty = frequency_penalty

    @property
    def logit_bias(self):
        """Gets the logit_bias of this CreateChatCompletionRequest.  # noqa: E501

        Modify the likelihood of specified tokens appearing in the completion.  Accepts a JSON object that maps tokens (specified by their token ID in the tokenizer) to an associated bias value from -100 to 100. Mathematically, the bias is added to the logits generated by the model prior to sampling. The exact effect will vary per model, but values between -1 and 1 should decrease or increase likelihood of selection; values like -100 or 100 should result in a ban or exclusive selection of the relevant token.   # noqa: E501

        :return: The logit_bias of this CreateChatCompletionRequest.  # noqa: E501
        :rtype: dict(str, int)
        """
        return self._logit_bias

    @logit_bias.setter
    def logit_bias(self, logit_bias):
        """Sets the logit_bias of this CreateChatCompletionRequest.

        Modify the likelihood of specified tokens appearing in the completion.  Accepts a JSON object that maps tokens (specified by their token ID in the tokenizer) to an associated bias value from -100 to 100. Mathematically, the bias is added to the logits generated by the model prior to sampling. The exact effect will vary per model, but values between -1 and 1 should decrease or increase likelihood of selection; values like -100 or 100 should result in a ban or exclusive selection of the relevant token.   # noqa: E501

        :param logit_bias: The logit_bias of this CreateChatCompletionRequest.  # noqa: E501
        :type: dict(str, int)
        """

        self._logit_bias = logit_bias

    @property
    def logprobs(self):
        """Gets the logprobs of this CreateChatCompletionRequest.  # noqa: E501

        Whether to return log probabilities of the output tokens or not. If true, returns the log probabilities of each output token returned in the `content` of `message`.  # noqa: E501

        :return: The logprobs of this CreateChatCompletionRequest.  # noqa: E501
        :rtype: bool
        """
        return self._logprobs

    @logprobs.setter
    def logprobs(self, logprobs):
        """Sets the logprobs of this CreateChatCompletionRequest.

        Whether to return log probabilities of the output tokens or not. If true, returns the log probabilities of each output token returned in the `content` of `message`.  # noqa: E501

        :param logprobs: The logprobs of this CreateChatCompletionRequest.  # noqa: E501
        :type: bool
        """

        self._logprobs = logprobs

    @property
    def top_logprobs(self):
        """Gets the top_logprobs of this CreateChatCompletionRequest.  # noqa: E501

        An integer between 0 and 20 specifying the number of most likely tokens to return at each token position, each with an associated log probability. `logprobs` must be set to `true` if this parameter is used.  # noqa: E501

        :return: The top_logprobs of this CreateChatCompletionRequest.  # noqa: E501
        :rtype: int
        """
        return self._top_logprobs

    @top_logprobs.setter
    def top_logprobs(self, top_logprobs):
        """Sets the top_logprobs of this CreateChatCompletionRequest.

        An integer between 0 and 20 specifying the number of most likely tokens to return at each token position, each with an associated log probability. `logprobs` must be set to `true` if this parameter is used.  # noqa: E501

        :param top_logprobs: The top_logprobs of this CreateChatCompletionRequest.  # noqa: E501
        :type: int
        """

        self._top_logprobs = top_logprobs

    @property
    def max_tokens(self):
        """Gets the max_tokens of this CreateChatCompletionRequest.  # noqa: E501

        The maximum number of [tokens](/tokenizer) that can be generated in the chat completion.  The total length of input tokens and generated tokens is limited by the model's context length. [Example Python code](https://cookbook.openai.com/examples/how_to_count_tokens_with_tiktoken) for counting tokens.   # noqa: E501

        :return: The max_tokens of this CreateChatCompletionRequest.  # noqa: E501
        :rtype: int
        """
        return self._max_tokens

    @max_tokens.setter
    def max_tokens(self, max_tokens):
        """Sets the max_tokens of this CreateChatCompletionRequest.

        The maximum number of [tokens](/tokenizer) that can be generated in the chat completion.  The total length of input tokens and generated tokens is limited by the model's context length. [Example Python code](https://cookbook.openai.com/examples/how_to_count_tokens_with_tiktoken) for counting tokens.   # noqa: E501

        :param max_tokens: The max_tokens of this CreateChatCompletionRequest.  # noqa: E501
        :type: int
        """

        self._max_tokens = max_tokens

    @property
    def n(self):
        """Gets the n of this CreateChatCompletionRequest.  # noqa: E501

        How many chat completion choices to generate for each input message. Note that you will be charged based on the number of generated tokens across all of the choices. Keep `n` as `1` to minimize costs.  # noqa: E501

        :return: The n of this CreateChatCompletionRequest.  # noqa: E501
        :rtype: int
        """
        return self._n

    @n.setter
    def n(self, n):
        """Sets the n of this CreateChatCompletionRequest.

        How many chat completion choices to generate for each input message. Note that you will be charged based on the number of generated tokens across all of the choices. Keep `n` as `1` to minimize costs.  # noqa: E501

        :param n: The n of this CreateChatCompletionRequest.  # noqa: E501
        :type: int
        """

        self._n = n

    @property
    def presence_penalty(self):
        """Gets the presence_penalty of this CreateChatCompletionRequest.  # noqa: E501

        Number between -2.0 and 2.0. Positive values penalize new tokens based on whether they appear in the text so far, increasing the model's likelihood to talk about new topics.  [See more information about frequency and presence penalties.](/docs/guides/text-generation/parameter-details)   # noqa: E501

        :return: The presence_penalty of this CreateChatCompletionRequest.  # noqa: E501
        :rtype: float
        """
        return self._presence_penalty

    @presence_penalty.setter
    def presence_penalty(self, presence_penalty):
        """Sets the presence_penalty of this CreateChatCompletionRequest.

        Number between -2.0 and 2.0. Positive values penalize new tokens based on whether they appear in the text so far, increasing the model's likelihood to talk about new topics.  [See more information about frequency and presence penalties.](/docs/guides/text-generation/parameter-details)   # noqa: E501

        :param presence_penalty: The presence_penalty of this CreateChatCompletionRequest.  # noqa: E501
        :type: float
        """

        self._presence_penalty = presence_penalty

    @property
    def response_format(self):
        """Gets the response_format of this CreateChatCompletionRequest.  # noqa: E501


        :return: The response_format of this CreateChatCompletionRequest.  # noqa: E501
        :rtype: CreateChatCompletionRequestResponseFormat
        """
        return self._response_format

    @response_format.setter
    def response_format(self, response_format):
        """Sets the response_format of this CreateChatCompletionRequest.


        :param response_format: The response_format of this CreateChatCompletionRequest.  # noqa: E501
        :type: CreateChatCompletionRequestResponseFormat
        """

        self._response_format = response_format

    @property
    def seed(self):
        """Gets the seed of this CreateChatCompletionRequest.  # noqa: E501

        This feature is in Beta. If specified, our system will make a best effort to sample deterministically, such that repeated requests with the same `seed` and parameters should return the same result. Determinism is not guaranteed, and you should refer to the `system_fingerprint` response parameter to monitor changes in the backend.   # noqa: E501

        :return: The seed of this CreateChatCompletionRequest.  # noqa: E501
        :rtype: int
        """
        return self._seed

    @seed.setter
    def seed(self, seed):
        """Sets the seed of this CreateChatCompletionRequest.

        This feature is in Beta. If specified, our system will make a best effort to sample deterministically, such that repeated requests with the same `seed` and parameters should return the same result. Determinism is not guaranteed, and you should refer to the `system_fingerprint` response parameter to monitor changes in the backend.   # noqa: E501

        :param seed: The seed of this CreateChatCompletionRequest.  # noqa: E501
        :type: int
        """

        self._seed = seed

    @property
    def stop(self):
        """Gets the stop of this CreateChatCompletionRequest.  # noqa: E501

        Up to 4 sequences where the API will stop generating further tokens.   # noqa: E501

        :return: The stop of this CreateChatCompletionRequest.  # noqa: E501
        :rtype: OneOfCreateChatCompletionRequestStop
        """
        return self._stop

    @stop.setter
    def stop(self, stop):
        """Sets the stop of this CreateChatCompletionRequest.

        Up to 4 sequences where the API will stop generating further tokens.   # noqa: E501

        :param stop: The stop of this CreateChatCompletionRequest.  # noqa: E501
        :type: OneOfCreateChatCompletionRequestStop
        """

        self._stop = stop

    @property
    def stream(self):
        """Gets the stream of this CreateChatCompletionRequest.  # noqa: E501

        If set, partial message deltas will be sent, like in ChatGPT. Tokens will be sent as data-only [server-sent events](https://developer.mozilla.org/en-US/docs/Web/API/Server-sent_events/Using_server-sent_events#Event_stream_format) as they become available, with the stream terminated by a `data: [DONE]` message. [Example Python code](https://cookbook.openai.com/examples/how_to_stream_completions).   # noqa: E501

        :return: The stream of this CreateChatCompletionRequest.  # noqa: E501
        :rtype: bool
        """
        return self._stream

    @stream.setter
    def stream(self, stream):
        """Sets the stream of this CreateChatCompletionRequest.

        If set, partial message deltas will be sent, like in ChatGPT. Tokens will be sent as data-only [server-sent events](https://developer.mozilla.org/en-US/docs/Web/API/Server-sent_events/Using_server-sent_events#Event_stream_format) as they become available, with the stream terminated by a `data: [DONE]` message. [Example Python code](https://cookbook.openai.com/examples/how_to_stream_completions).   # noqa: E501

        :param stream: The stream of this CreateChatCompletionRequest.  # noqa: E501
        :type: bool
        """

        self._stream = stream

    @property
    def stream_options(self):
        """Gets the stream_options of this CreateChatCompletionRequest.  # noqa: E501


        :return: The stream_options of this CreateChatCompletionRequest.  # noqa: E501
        :rtype: ChatCompletionStreamOptions
        """
        return self._stream_options

    @stream_options.setter
    def stream_options(self, stream_options):
        """Sets the stream_options of this CreateChatCompletionRequest.


        :param stream_options: The stream_options of this CreateChatCompletionRequest.  # noqa: E501
        :type: ChatCompletionStreamOptions
        """

        self._stream_options = stream_options

    @property
    def temperature(self):
        """Gets the temperature of this CreateChatCompletionRequest.  # noqa: E501

        What sampling temperature to use, between 0 and 2. Higher values like 0.8 will make the output more random, while lower values like 0.2 will make it more focused and deterministic.  We generally recommend altering this or `top_p` but not both.   # noqa: E501

        :return: The temperature of this CreateChatCompletionRequest.  # noqa: E501
        :rtype: float
        """
        return self._temperature

    @temperature.setter
    def temperature(self, temperature):
        """Sets the temperature of this CreateChatCompletionRequest.

        What sampling temperature to use, between 0 and 2. Higher values like 0.8 will make the output more random, while lower values like 0.2 will make it more focused and deterministic.  We generally recommend altering this or `top_p` but not both.   # noqa: E501

        :param temperature: The temperature of this CreateChatCompletionRequest.  # noqa: E501
        :type: float
        """

        self._temperature = temperature

    @property
    def top_p(self):
        """Gets the top_p of this CreateChatCompletionRequest.  # noqa: E501

        An alternative to sampling with temperature, called nucleus sampling, where the model considers the results of the tokens with top_p probability mass. So 0.1 means only the tokens comprising the top 10% probability mass are considered.  We generally recommend altering this or `temperature` but not both.   # noqa: E501

        :return: The top_p of this CreateChatCompletionRequest.  # noqa: E501
        :rtype: float
        """
        return self._top_p

    @top_p.setter
    def top_p(self, top_p):
        """Sets the top_p of this CreateChatCompletionRequest.

        An alternative to sampling with temperature, called nucleus sampling, where the model considers the results of the tokens with top_p probability mass. So 0.1 means only the tokens comprising the top 10% probability mass are considered.  We generally recommend altering this or `temperature` but not both.   # noqa: E501

        :param top_p: The top_p of this CreateChatCompletionRequest.  # noqa: E501
        :type: float
        """

        self._top_p = top_p

    @property
    def tools(self):
        """Gets the tools of this CreateChatCompletionRequest.  # noqa: E501

        A list of tools the model may call. Currently, only functions are supported as a tool. Use this to provide a list of functions the model may generate JSON inputs for. A max of 128 functions are supported.   # noqa: E501

        :return: The tools of this CreateChatCompletionRequest.  # noqa: E501
        :rtype: list[ChatCompletionTool]
        """
        return self._tools

    @tools.setter
    def tools(self, tools):
        """Sets the tools of this CreateChatCompletionRequest.

        A list of tools the model may call. Currently, only functions are supported as a tool. Use this to provide a list of functions the model may generate JSON inputs for. A max of 128 functions are supported.   # noqa: E501

        :param tools: The tools of this CreateChatCompletionRequest.  # noqa: E501
        :type: list[ChatCompletionTool]
        """

        self._tools = tools

    @property
    def tool_choice(self):
        """Gets the tool_choice of this CreateChatCompletionRequest.  # noqa: E501


        :return: The tool_choice of this CreateChatCompletionRequest.  # noqa: E501
        :rtype: ChatCompletionToolChoiceOption
        """
        return self._tool_choice

    @tool_choice.setter
    def tool_choice(self, tool_choice):
        """Sets the tool_choice of this CreateChatCompletionRequest.


        :param tool_choice: The tool_choice of this CreateChatCompletionRequest.  # noqa: E501
        :type: ChatCompletionToolChoiceOption
        """

        self._tool_choice = tool_choice

    @property
    def user(self):
        """Gets the user of this CreateChatCompletionRequest.  # noqa: E501

        A unique identifier representing your end-user, which can help OpenAI to monitor and detect abuse. [Learn more](/docs/guides/safety-best-practices/end-user-ids).   # noqa: E501

        :return: The user of this CreateChatCompletionRequest.  # noqa: E501
        :rtype: str
        """
        return self._user

    @user.setter
    def user(self, user):
        """Sets the user of this CreateChatCompletionRequest.

        A unique identifier representing your end-user, which can help OpenAI to monitor and detect abuse. [Learn more](/docs/guides/safety-best-practices/end-user-ids).   # noqa: E501

        :param user: The user of this CreateChatCompletionRequest.  # noqa: E501
        :type: str
        """

        self._user = user

    @property
    def function_call(self):
        """Gets the function_call of this CreateChatCompletionRequest.  # noqa: E501

        Deprecated in favor of `tool_choice`.  Controls which (if any) function is called by the model. `none` means the model will not call a function and instead generates a message. `auto` means the model can pick between generating a message or calling a function. Specifying a particular function via `{\"name\": \"my_function\"}` forces the model to call that function.  `none` is the default when no functions are present. `auto` is the default if functions are present.   # noqa: E501

        :return: The function_call of this CreateChatCompletionRequest.  # noqa: E501
        :rtype: OneOfCreateChatCompletionRequestFunctionCall
        """
        return self._function_call

    @function_call.setter
    def function_call(self, function_call):
        """Sets the function_call of this CreateChatCompletionRequest.

        Deprecated in favor of `tool_choice`.  Controls which (if any) function is called by the model. `none` means the model will not call a function and instead generates a message. `auto` means the model can pick between generating a message or calling a function. Specifying a particular function via `{\"name\": \"my_function\"}` forces the model to call that function.  `none` is the default when no functions are present. `auto` is the default if functions are present.   # noqa: E501

        :param function_call: The function_call of this CreateChatCompletionRequest.  # noqa: E501
        :type: OneOfCreateChatCompletionRequestFunctionCall
        """

        self._function_call = function_call

    @property
    def functions(self):
        """Gets the functions of this CreateChatCompletionRequest.  # noqa: E501

        Deprecated in favor of `tools`.  A list of functions the model may generate JSON inputs for.   # noqa: E501

        :return: The functions of this CreateChatCompletionRequest.  # noqa: E501
        :rtype: list[ChatCompletionFunctions]
        """
        return self._functions

    @functions.setter
    def functions(self, functions):
        """Sets the functions of this CreateChatCompletionRequest.

        Deprecated in favor of `tools`.  A list of functions the model may generate JSON inputs for.   # noqa: E501

        :param functions: The functions of this CreateChatCompletionRequest.  # noqa: E501
        :type: list[ChatCompletionFunctions]
        """

        self._functions = functions

    def to_dict(self):
        """Returns the model properties as a dict"""
        result = {}

        for attr, _ in six.iteritems(self.swagger_types):
            value = getattr(self, attr)
            if isinstance(value, list):
                result[attr] = list(map(
                    lambda x: x.to_dict() if hasattr(x, "to_dict") else x,
                    value
                ))
            elif hasattr(value, "to_dict"):
                result[attr] = value.to_dict()
            elif isinstance(value, dict):
                result[attr] = dict(map(
                    lambda item: (item[0], item[1].to_dict())
                    if hasattr(item[1], "to_dict") else item,
                    value.items()
                ))
            else:
                result[attr] = value
        if issubclass(CreateChatCompletionRequest, dict):
            for key, value in self.items():
                result[key] = value

        return result

    def to_str(self):
        """Returns the string representation of the model"""
        return pprint.pformat(self.to_dict())

    def __repr__(self):
        """For `print` and `pprint`"""
        return self.to_str()

    def __eq__(self, other):
        """Returns true if both objects are equal"""
        if not isinstance(other, CreateChatCompletionRequest):
            return False

        return self.__dict__ == other.__dict__

    def __ne__(self, other):
        """Returns true if both objects are not equal"""
        return not self == other
